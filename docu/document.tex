\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{float}
\usepackage{textcomp}

\usetikzlibrary{positioning,fit,calc,arrows.meta, shapes}
\graphicspath{ {images/} }

%Tot això hauria d'anar en un pkg, però no sé com és fa
\newcommand*{\assignatura}[1]{\gdef\1assignatura{#1}}
\newcommand*{\grup}[1]{\gdef\3grup{#1}}
\newcommand*{\professorat}[1]{\gdef\4professorat{#1}}
\renewcommand{\title}[1]{\gdef\5title{#1}}
\renewcommand{\author}[1]{\gdef\6author{#1}}
\renewcommand{\date}[1]{\gdef\7date{#1}}
\renewcommand{\maketitle}{ %fa el maketitle de nou
    \begin{titlepage}
        \raggedright{UNIVERSITAT DE LLEIDA \\
            Escola Politècnica Superior \\
            Grau en Enginyeria Informàtica\\
            \1assignatura\\}
            \vspace{5cm}
            \centering\huge{\5title \\}
            \vspace{3cm}
            \large{\6author} \\
            \normalsize{\3grup}
            \vfill
            Professorat : \4professorat \\
            Data : \7date
\end{titlepage}}
%Emplenar a partir d'aquí per a fer el títol : no se com es fa el package
%S'han de renombrar totes, inclús date, si un camp es deixa en blanc no apareix

\tikzset{
	%Style of nodes. Si poses aquí un estil es pot reutilitzar més facilment
	pag/.style = {circle, draw=black,
                           minimum width=0.75cm, font=\ttfamily,
                           text centered}
}
\renewcommand{\figurename}{Figura}
\title{Tercera pràctica d'Intel·ligència Artificial}
\author{Ian Palacín Aliana i Joaquim Picó Mora}
\date{Diumenge 12 de Gener}
\assignatura{Inteligència Artificial}
\professorat{Jesús Ojeda}
\grup{}

%Comença el document
\begin{document}
\maketitle
\thispagestyle{empty}

\newpage
\pagenumbering{roman}
\tableofcontents
\newpage
\pagenumbering{arabic}
%

\section{Árboles de decisión}
\subsection{Construcción del árbol de forma recursiva}
Para construir un buen árbol, es necesario
dividir los sets en base a todas las preguntas y respuestas
possibles, y quedarse sólo con aquellos que maximicen el augmento
de impureza. Para hacerlo, hemos recurrido a una implementación
recursiva que hace lo mencionado.\\
Este método, dada una partición, una función de puntuación (en nuestro
caso, gini o entropía) y una beta (mencionada más adelante), retorna
el nodo raíz de nuestro árbol de decisión.\\
Para hacerlo, itera de forma que formula cada pregunta con cada elemento
del dominio de esa pregunta, y divide los sets en base a eso. Una vez divide
los sets calcula la ganancia de impureza, si es la mejor hasta el momento se
guarda los parámetros de la partición (pregunta, ganancia y sets) y sigue 
con las demás preguntas.\\
Como podemos observar, cuando se acaba este proceso, tenemos la mejor
partición de sets posible, es entonces cuando usamos el parámetro beta. Si
la ganancia de impureza es menor que beta, paramos de construir el árbol, y el 
nodo en el que estamos se convertirá en hoja. En
caso contrario, seguimos construyendo el árbol con las dos particiones obtenidas.
De esa forma, se puede buscar una beta tal que el árbol resultante no sea demasiado
grande. Ya veremos que no es la mejor forma de podar el árbol.
%
\subsection{Construcción del árbol de forma iterativa}
%
Este método, igual que el anterior, crea un arbol de decisión pero
esta vez se tiene que construir de forma iterativa. Para hacer-lo, se
ha implementado con un bucle que trabaja siempre que haya elementos en 
una lista. Esta lista será una pila donde cada partición realizada
se le añadirá una estructura de datos contenida por el dataset despues
de la partición, el nodo responsable de la partición i si es la rama
true o false. El hecho de guardar en la estructura el dataset juntamente con 
el nodo padre i la rama servirá para poder relacionar a posteriori los nodos
que conformaran el arbol.\\
Dentro de este bucle se realiza lo mismo que en la función recursiva, se busca
el elemento con el cual se consiga el mayor gain al dividir el dataset i
se crea el nodo a partir de este elemento.\\
Este nodo se introducirá en una lista donde iremnos guardando todos los 
nodos de decision que se crean junto con el nodo del cual derivan 
i la rama sobre la cual trabajan. Esta será la lista desde la qual se 
relacionaran los nodos i se terminará de construir el arbol.
\subsection{Función de clasificación}
%
Classify se ha pensado como una función recursiva que partiendo del nodo raíz
del arbol se va moviendo por las ramas hasta llegar a un nodo hoja en función
a los valores que tenga el nuevo objeto a classificar. Cuando se llega a un nodo 
hojase retorna su resultado.
\subsection{Evaluación del árbol}
%
Se puede llevar a cabo la evaluación del arbol de dos formas diferentes.\\ 
Una de ellas es dividiendo el dataset de forma manual en dos fitxeros, 
training\_set.data i test\_set.data i la otra és usando la función split\_set, 
la qual divide el dataset en dos partes con los objetos distribuidos de forma 
random. Esta función recive como parametros un dataset i un porcentaje. Este 
último és el porcentaje que queremos que tenga training\_set respecto del data\_set 
al completo.\\
Si ejecutamos la función test performance con el traning\_set a 70\% i a 90\%, 
vemos que el porcentaje de aciertos de la ejecución con 90\% practicamente siempre 
és mayor. Esto se deve a que nuestra IA ha sido entrenada con mayor volumen de datos, 
i por lo tanto puede realizar una mejor classificación.
\subsection{Missing data}
\subsection{Poda del árbol}
Podar el árbol utilizando beta no es la mejor
forma, ya que se podan los dos árboles hijo a la vez,
y se podria dar el caso de que el cambio de impureza de 
uno de ellos sea bueno (y por lo tato, el otro muy malo).
El método de poda empleado es el "bottom-up", que 
requiere de construir completamente el árbol primero.
Una vez construido para cada par de hojas con un mismo
padre, se mira si la unión de ellas aumenta la entropía
bajo un umbral dado (threshold). Si es así, se unen las 
dos hojas en el padre, en caso contrario se dejan las particiones
como estaban. Este proceso se repite hasta que no es posible
unir más hojas.\\
Para facilitar la implementación, hemos añadido una variable
a cada nodo llamada gain, que para cada nodo no hoja guarda 
la ganancia de impureza debido a la partición.\\ 
Originalmente se había implementado un método que podaba las hojas
en ese momento, y se tenía que repetir tantas veces como particiones 
a podar en una misma rama (para la rama más larga), resultando en un 
algoritmo de coste O(n*n).
En una segunda implementación, mediante una función recursiva se ha 
conseguido paliar el coste hasta O(n).

\section{Clustering}
\subsection{Restarting policies}
Dado el factor de aleatoriedad al inicializar los centroides,
es posible que se acaben consiguiendo clusters que no son 
realmente representativos, para ello se ha implementado las
restarting policies.\\
Antes de todo debemos saber una forma de medir cómo de buena
es una disposición de clusters dada, eso lo hacemos con la
función calc\_total\_dist, con la que dados los ítems y los 
centroides (sus vectores), calculamos la suma de las distancias
entre cada ítem y su centroide asociado. De esa forma podemos medir
quantitativamente cómo de bueno es una disposición de centroides 
respecto a otra.\\
Una vez que tenemos el sistema de qualificación, para buscar la mejor
disposición repetimos el proceso de k-means I veces, guardándonos
la mejor disposición hasta el momento. Una vez el proceso ha acabado,
obtenemos como resultado la mejor disposición de centroides con ítems
asociados de entre todas las iteraciones hechas. La función que hace este
trabajo se llama search\_cluster(I), donde I es una variable parametrizada 
del número de veces que se desea repetir el proceso de k-means. 

\end{document}

